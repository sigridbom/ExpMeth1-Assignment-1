---
title: "Portfolio 3 - The Reading Experiment"
author: "Sigrid"
date: "11/7/2019"
output: html_document
---

First I load the relevant packages and import my data from the reading experiment and the data from the MRC database. I will use the MRC database in order to determine the frequency of the words used in the experiment according to normal English. 
. 
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, pastecs, WRS2, stringr, stringi)

mrc_data <- read_csv("MRC_database.csv")

files <- list.files(path = "logfiles",     
                    pattern = ".csv",  
                    full.names = T) 
data <- lapply(files, read_csv) %>% plyr::rbind.fill()

#fixing gender with both upper and lower case due to a prior error in the code 
data$Gender <- tolower(data$Gender)

```

# Part 1 - Which properties of words correlate with word-by-word reading times?

First I will investigate whether the data are normally distributed or not. If they are normally distributed I can calculate the correlation with parametric tests, and if not I will use non-parametric tests, i.e. Spearman's Rho or Kendall's Tau. 
I will subset the data into the two different conditions because the data sets represent two different groups.  

### Checking assumptions
```{r checking assumptions}

#subsetting the data into the two conditions

con1 <- data %>% filter(data$Version == "stream.")
con2 <- data %>% filter(data$Version == "building.")

### histogram condition 1 (expected word)
con1_hist <- ggplot(con1, aes(x = Reaction_time)) 
con1_hist + geom_histogram(aes(y = ..density..), binwidth = 0.005) + stat_function(fun = dnorm, args = list(mean = mean(con1$Reaction_time), sd = sd(con1$Reaction_time)), colour = "blue", size = 1) +
  theme_minimal() + labs(x = "Reaction time", y = "Density") + ggtitle("Condition 1 reading time distribution")

## qq-plot for condition 1
qq_plot_con1 <- ggplot(con1, aes(sample = Reaction_time)) 
qq_plot_con1 + stat_qq() + stat_qq_line(colour = "blue") + ggtitle("Condition 1 reading time qq-plot")
  

### histogram condition 2 (unexpected word)
con2_hist <- ggplot(con2, aes(x = Reaction_time)) 
con2_hist + geom_histogram(aes(y = ..density..), binwidth = 0.005) + stat_function(fun = dnorm, args = list(mean = mean(con2$Reaction_time), sd = sd(con2$Reaction_time)), colour = "red", size = 1) +
  theme_minimal() + labs(x = "Reaction time", y = "Density") + ggtitle("Condition 2 reading time distribution")

## qq-plot for condition 2
qq_plot_con2 <- ggplot(con2, aes(sample = Reaction_time)) 
qq_plot_con2 + stat_qq() + stat_qq_line(colour = "red") + ggtitle("Condition 2 reading time qq-plot")
  
#checking numeric values for both conditions
round(pastecs::stat.desc(cbind(con1$Reaction_time, con2$Reaction_time), basic = F, norm = TRUE), digits = 2)

```

The histograms and the QQ-plots do not look normally distributed. The values of skew.2SE and kurt.2SE for condition 1 are too high for a normal distribution (in this case 35.60 and 178.22 respectively). The same applies for condition 2 but with the values 78.10 (skew.2SE) and 614.23 (kurt.2SE). 
Because both distributions have a positive value of kurt.2SE they are leptokurtic, which is not normal.

The value Shapiro Wilk's test for condition 1 is 0.77 with a significant p-value (p<0,05), which means that the data are significantly different from a normal distribution. The same applies for condition 2 except the value of the test is 0.56.

The visual and the numeric outcome strongly indicates that the data are not normally distributed. 
I will now try to transform the data by removing outliers and logging the reaction time. 

### Transforming the data
```{r transforming the data - removing outliers and logging the data}
#finding z-scores
data$z <- (data$Reaction_time - mean(data$Reaction_time))/sd(data$Reaction_time)

#filtering the dataframe for z-scores between -3 and 3
data_out <- filter(data, data$z > -3 & data$z < 3)

#filtering the two subsets
con1_out <- data_out %>% filter(data_out$Version == "stream.")
con2_out <- data_out %>% filter(data_out$Version == "building.")

#logging the data for both conditions
con1_out_log <- con1_out %>% mutate(rt_log = log(con1_out$Reaction_time), 
                                    rt_sqrt = sqrt(con1_out$Reaction_time), 
                                    rt_inv = 1/con1_out$Reaction_time)

con2_out_log <- con2_out %>% mutate(rt_log = log(con2_out$Reaction_time), 
                                    rt_sqrt = sqrt(con2_out$Reaction_time), 
                                    rt_inv = 1/con2_out$Reaction_time)

#checking numeric values for both conditions with the logged data
round(pastecs::stat.desc(cbind(con1_out_log$rt_log, con1_out_log$rt_sqrt, con1_out_log$rt_inv, con2_out_log$rt_log, con2_out_log$rt_sqrt, con2_out_log$rt_inv), basic = F, norm = TRUE), digits = 2)

```
I have made new histograms and qq-plots (but will not show them here), and they still do not look normally distributed. 

The values of skew.2SE and kurt.2SE are not acceptable for a normal distribution.
The Shapiro Wilk's test implies that the distributions are significantly different from a normal distribution (p<0.05). 

I will therefore conclude that the assumptions are violated and the data are not normally distributed.

An explanation for this could be that a normal distribution allows negative values and extends to very large and small data points, whereas the crucial value in my data set (reaction time) is measured in seconds which cannot be measured negatively. 

I will therefore continue with non-parametric tests for correlation on my untransformed data. 
I will remove the outliers though, because they can cause unsystematic variance. 

I will use Kendalls' Tau to test for correlation because our data set is fairly small (<30 logfiles).

## Correlation between reaction time and word length
```{r word length and correlation - devide the data into subset with the different condition}

#removing punctuation
data_out <- mutate(data_out, Stimulus = str_replace_all(data_out$Stimulus, "[:punct:]", ""))

#making a new column with the lenght of the word
data_out$word_len <- nchar(data_out$Stimulus)

#correlation test
cor.test(data_out$word_len, data_out$Reaction_time, method = "kendall")

#visualize 
scat_len <- ggplot(data_out, aes(word_len,Reaction_time))
scat_len + geom_point() + geom_smooth(method = lm, colour = "red") + ggtitle("Correlation between word length and reaction time") + labs(x = "Word length (number of letters)", y = "Reaction time (seconds)")

```

A Kendall's Tau value of 1 indicates a perfect positive correlation, where a value of -1 indicates a perfect negative correlation. A value of 0 indicates no correlation. 

The value of Kendall's Tau is 0.05 with a significant p-value<0.01. 
Even though the results are significant the effect size is so small that the correlation is not very strong. 

## Correlation between reaction time and word frequency
```{r word frequency}

data_out$word <- toupper(data_out$Stimulus)

df_merge <- merge(data_out, mrc_data, by = "word")

cor.test(df_merge$kf_freq, df_merge$Reaction_time, method = "kendall")

#visualize 
scat_freq <- ggplot(df_merge, aes(kf_freq, Reaction_time))
scat_freq + geom_point() + geom_smooth(method = lm, colour = "red") + ggtitle("Correlation between word frequency and reaction time")+ labs(x = "Word frequency (MRC database)", y = "Reaction time (seconds)")

```

The value of tau is -0,06 with a significant p-value (p<0.01).
The results are significant, but with a very small effect size. 

## Correlation between reaction time and word number
```{r word number}

cor.test(data_out$X1, data_out$Reaction_time, method = "kendall")

#visualize 
scat_num <- ggplot(data_out, aes(X1,Reaction_time))
scat_num + geom_point() + geom_smooth(method = lm, colour = "red") + ggtitle("Correlation between word number and reaction time") + labs(x = "Word number in text (ordinal)", y = "Reaction time (seconds)")

```


The value of Tau is -0.1 rounded up with a significant p-value (p<0.01). This means that there is a small negative correlation between the reaction time and the order of the words in the story. 

In conclusion all three word properties actually have a significant correlation with reaction time, but it is only the word number i.e. the order of words in the story that has a small effect size, where the two others each have an effect size that is too small to conclude there is a meaningful correlation. 

# Part 2 - How do semantic-contextual expectations affect reading times?

First I will check the assumptions again because we now have four groups of data; the two versions and within those the expected or the unexpected word (i.e. word number 106) and the word following that (word number 107).

### Checking assumptions again
```{r checking assumptions again}

con1_out_w1 <- filter(con1_out, con1_out$X1 == "106")
con2_out_w1 <- filter(con2_out, con2_out$X1 == "106")
con1_out_w2 <- filter(con1_out, con1_out$X1 == "107")
con2_out_w2 <- filter(con2_out, con2_out$X1 == "107")

round(pastecs::stat.desc(cbind(con1_out_w1$Reaction_time, con2_out_w1$Reaction_time, con1_out_w2$Reaction_time, con2_out_w2$Reaction_time), basic = F, norm = TRUE), digits = 2)


```

The values of skew.2SE and kurt.2SE for all four data sets are all between [-1;1] which supports a normal distribution. 
The Shapiro Wilk's test has values around 0,95 for all of the data sets with a not significant value of p (p>0.05), which means that the distributions of the four data sets could be normally distributed.

I have checked the qq-plots for all four data sets. They almost look normally distributed, so I will continue with a t-test that assumes the data are parametric (even though I do not have a lot of data points).

## T-test

```{r t-test parametric}

#First I will make two data sets so I can compare the two different conditions to firstly the normal/salient word and secondly the following word

data_106 <- filter(data_out, data_out$X1 == 106)
data_107 <- filter(data_out, data_out$X1 == 107)

#t-test
t.test(Reaction_time ~ Version, data = data_106)
t.test(Reaction_time ~ Version, data = data_107)

#calculating the standard deviation for each of the data sets
sd(con1_out_w1$Reaction_time)
sd(con1_out_w2$Reaction_time)
sd(con2_out_w1$Reaction_time)
sd(con2_out_w2$Reaction_time)

```

## Comparing the reaction time of the two conditions according to the expected and the unexpected word

When conducting an independent t-test I have found that the average reading time for the expected and unexpected word is not significantly increased by the unexpected word, t(23.98) = -0.63, p>0.05, (mean expected = 0.57, mean unexpected = 0.50, SD expected = 0.33, SD unexpected = 0.23)

The same goes for the word following the expected and unexpected word, t(22.62) = 1.01, p>0.05, (mean expected = 0.48, mean unexpected = 0.56, SD expected = 0.21 and SD unexpected = 0.23).

I can therefore conclude that there is no difference in reaction time between the two versions of the text. 

## Visualizing the t-tests
```{r visualizing the t-tests}

boxplot_106 <- ggplot(data_106, aes(Version, Reaction_time, colour = Version))
boxplot_106 + geom_boxplot(width = 0.5) + stat_summary(fun.y = mean, geom = "point") + labs(y = "Reaction time") + coord_cartesian(xlim = NULL, ylim = c(1.5,0)) + ggtitle("The salient/not salient word") 

boxplot_107 <- ggplot(data_107, aes(Version, Reaction_time, colour = Version))
boxplot_107 + geom_boxplot(width = 0.5) + stat_summary(fun.y = mean, geom = "point") + labs(y = "Reaction time") + coord_cartesian(xlim = NULL, ylim = c(1.5,0)) + ggtitle("The word following the salient/not salient word")

```

Here it should be noted that the word "stream" represents condition 1 (the expected word) and the word "building" represents condition 2 (the unexpected word). 
