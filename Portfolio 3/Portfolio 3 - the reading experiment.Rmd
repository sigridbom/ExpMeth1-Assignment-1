---
title: "Portfolio 3 - The Reading Experiment"
author: "Sigrid"
date: "11/7/2019"
output: html_document
---

First I load the relevant packages and import my data from the reading experiment and the data from the MRC database. I will use the MRC database in order to determine the frequency of the words used in the experiment according to normal English. 
. 
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, pastecs, WRS2, stringr, stringi)

mrc_data <- read_csv("MRC_database.csv")

files <- list.files(path = "logfiles",     
                    pattern = ".csv",  
                    full.names = T) 
data <- lapply(files, read_csv) %>% plyr::rbind.fill()

#fixing gender with both upper and lower case due to differences in the code in the group
data$Gender <- tolower(data$Gender)

```

#Part 1 - Which properties of words correlate with word-by-word reading times?

First I will investigate whether the data are normally distributed or not. If they are normally distributed I can calculate the correlation with parametric tests, and if not I will use non-parametric tests, i.e. Spearman's Rho or Kendall's Tau. 

```{r checking assumptions}

#creating a histogram
reac_hist <- ggplot(data, aes(Reaction_time))
reac_hist + geom_histogram(aes(y=..density..), binwidth = 0.8) + stat_function(fun = dnorm, args = list(mean(data$Reaction_time), sd = sd(data$Reaction_time)), colour = "red", size = 1) + theme_minimal()

#creating a qqplot
qq_plot <- ggplot(data, aes(sample = Reaction_time)) 
qq_plot + stat_qq() + stat_qq_line(colour = "red")

#checking numeric values
round(pastecs::stat.desc(data$Reaction_time, basic = F, norm = TRUE), digits = 2)

```

The histogram (where the red line shows what that would look like if it was normally distributed) is not looking normally distributed at all. The QQ-plot looks a little bit better but it is still not good. The values of skew.2SE and kurt.2SE are extremely high, 72,80 and 467,70 respectively. 
The Shapiro-Wilk test has a value of 0,67 with a corresponding p-value of 0,00. This implies that the data are significantly different than a normal distrubtion.

The visual and the numeric outcome strongly indicates that the data are not normally distributed. 
I will now try to transform the data by removing outliers and logging reaction time. 


```{r transforming the data - outliers}
#finding z-scores
data$z <- (data$Reaction_time - mean(data$Reaction_time))/sd(data$Reaction_time)

#filtering the dataframe for z-scores between -3 and 3
data_out <- filter(data, data$z <=3 | data$z <=-3)

#logging the data
data_log <- data_out %>% mutate(log(data_out$Reaction_time))

#making a new histogram
reac_hist2 <- ggplot(data_log, aes(Reaction_time))
reac_hist2 + geom_histogram(aes(y=..density..), binwidth = 0.8) + stat_function(fun = dnorm, args = list(mean(data$Reaction_time), sd = sd(data$Reaction_time)), colour = "red", size = 1) + theme_minimal()

#making a new qqplot
qq_plot2 <- ggplot(data_log, aes(sample = Reaction_time)) 
qq_plot2 + stat_qq() + stat_qq_line(colour = "red")

#checking the numeric values
round(pastecs::stat.desc(data_log$Reaction_time, basic = F, norm = TRUE), digits = 2)


```

It helped, but the data are still not looking normally distributed. The values of skew.2SE and kurt.2SE are now 11,45 and 7,02 respectively. It looks negatively skewed, and because of the postive value of kurt.2SE possible also leptokurtic.

The assumptions are violated and I conclude that the data are not normally distributed.

An explanation for this could be that a normal distribution allows negative values and extends to very large and small data points, whereas the crucial value in my data set (reaction time) is measured in seconds which cannot be measured negatively. 

I will therefore continue with non-parametric tests for correlation on my untransformed data. I will remove the outliers though, because they can cause irrelevant variance. 

I will use Kendalls' Tau to test for correlation because our data set is fairly small (<30 logfiles).

## Correlation between reaction time and word length
```{r word length and correlation}

#removing punctuation
data <- mutate(data_out, Stimulus = str_replace_all(data_out$Stimulus, "[:punct:]", ""))

#making a new column with the lenght of the word
data_out$word_len <- nchar(data_out$Stimulus)

#correlation test
cor.test(data_out$word_len, data_out$Reaction_time, method = "kendall")

#visualize 
scat1 <- ggplot(data_out, aes(word_len,Reaction_time))
scat1 + geom_point() + geom_smooth(method = lm, colour = "red")

```
A Kendall's Tau value of 1 indicates a perfect positive correlation, where a value of -1 indicates a perfect negative correlation. A value of 0 indicates no correlation. 

The value of Kendall's Tau is 0,05 with a significant p-value<0,05. 
Even though the results are significant the effect size is so small that the correlation is not very strong. 

##Correlation between reaction time and word frequency
```{r word frequency}

data_out$word <- toupper(data_out$Stimulus)

df_merge <- merge(data_out, mrc_data, by = "word")

cor.test(df_merge$kf_freq, df_merge$Reaction_time, method = "kendall")

#visualize 
scat2 <- ggplot(df_merge, aes(kf_freq, Reaction_time))
scat2 + geom_point() + geom_smooth(method = lm, colour = "red")

```
The value of tau is -0,06 with a significant p-value (p<0,05).
The results are significant, but with a very small effect size. 

## Correlation between reaction time and word number
```{r word number}

cor.test(data_out$X1, data_out$Reaction_time, method = "kendall")

#visualize 
scat3 <- ggplot(data_out, aes(X1,Reaction_time))
scat3 + geom_point() + geom_smooth(method = lm, colour = "red")

```

The value of tau is -0,1 rouned up with a significant p-value. This means that there is a small effect in the correlation between the reaction time and the order of the words in the story. 

This makes sense because 

Conclusion:



#Part 2 - How do semantic-contextual expectations affect reading times?

To the fun part. 

```{r checking assumptions again}


data_con1 <- filter(data_out, data_out$Version == "stream.")
data_con2 <- filter(data_out, data_out$Version == "building.")
data_con1_w2 <- filter(data_con1, data_con1$X1 == "107")
data_con2_w2 <- filter(data_con2, data_con2$X1 == "107")

round(pastecs::stat.desc(cbind(data_con1$Reaction_time, data_con2$Reaction_time, data_con1_w2$Reaction_time, data_con2_w2$Reaction_time), basic = F, norm = TRUE), digits = 2)


```


```{r delete}

#stat.desc on four different things?? version and word 106 or 107

#data_con_1 <- filter(data, data$X1 == c(106, 107), data$Version == "stream.")
#data_con_2 <- filter(data, data$X1 == c(106, 107), data$Version == "building.")

```


```{r t-test non-parametric}

data_106 <- filter(data_out, data$X1 == 106)
data_107 <- filter(data_out, data$X1 == 107)

WRS2::yuen(Reaction_time ~ Version, data = data_106)
WRS2::yuen(Reaction_time ~ Version, data = data_107)

```


```{r t-test Welch}


data_106_log <- data_106 %>% mutate(log(data_106$Reaction_time))
data_107_log <- data_107 %>% mutate(log(data_107$Reaction_time))



t.test(Reaction_time ~ Version, data = data_106_log)

t.test(Reaction_time ~ Version, data = data_107_log)

```

```{r visualizing reaction time}

plot5 <- ggplot(data_log, aes(Version, Reaction_time), fill = version)
plot5 + geom_bar() 

#qqplot???

```


NOTES
Non-parametric test (not normally distributed) Kendalls Tau and Spearman's Rho
Covariance
#Fabio
Word frequency - mrc database

t.test
two seperate tests for each of the conditions (building and stream vs the following word from the two conditions). number something for "they"
keep it minimalistic

LÆS OM FORSKEL på kendall og spearman (ordinal variables? - see class 5 lecture slides)

#QUESTIONS FOR TELMA

- should I do stat.desc on my transformed data or the original data?? BOTH
- Is it a good idea to do the t-test for non-parametric data and then transform the data and do a normal t-test to see the difference?  yes
- does it make a difference that we have unequal sample sizes? (13 vs 15)  note in discussion 
- Twitter setup!!
# second part assumption checking with two words for each condition